{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31hIf_TzxXH9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Clustering\n",
        "\n",
        "\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "\n",
        "class BaseClustering:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit_predict(self, X):\n",
        "        pass\n",
        "\n",
        "    def score(self, X, y_true):\n",
        "        y_pred = self.fit_predict(X)\n",
        "        pairwise_precision = self._calc_pw_precision(y_pred, y_true)\n",
        "        pairwise_recall = self._calc_pw_recall(y_pred, y_true)\n",
        "        pairwise_f_measure = 2 * (pairwise_precision * pairwise_recall)\\\n",
        "            / (pairwise_precision + pairwise_recall)\n",
        "        return pairwise_f_measure, pairwise_precision, pairwise_recall\n",
        "\n",
        "    @staticmethod\n",
        "    def _calc_pw_precision(y_pred, y_true):\n",
        "        unique_clusters = np.unique(y_pred)\n",
        "        n_pairs = 0\n",
        "        n_same_class_pairs = 0\n",
        "        for cluster in unique_clusters:\n",
        "            sample_indices = np.where(y_pred == cluster)[0]\n",
        "            combs = np.array(list(itertools.combinations(sample_indices, 2)), dtype=np.int64)\n",
        "            if not np.any(combs):\n",
        "                continue\n",
        "            combs_classes = y_true[combs]\n",
        "            same_class_pairs = np.where(combs_classes[:, 0] == combs_classes[:, 1])[0]\n",
        "            n_pairs += len(combs)\n",
        "            n_same_class_pairs += len(same_class_pairs)\n",
        "        pw_precision = n_same_class_pairs / n_pairs\n",
        "        return pw_precision\n",
        "\n",
        "    @staticmethod\n",
        "    def _calc_pw_recall(y_pred, y_true):\n",
        "        unique_classes = np.unique(y_true)\n",
        "        n_pairs = 0\n",
        "        n_same_cluster_pairs = 0\n",
        "        for clss in unique_classes:\n",
        "            sample_indices = np.where(y_true == clss)[0]\n",
        "            combs = np.array(list(itertools.combinations(sample_indices, 2)), dtype=np.int64)\n",
        "            if not np.any(combs):\n",
        "                continue\n",
        "            combs_clusters = y_pred[combs]\n",
        "            same_cluster_pairs = np.where(combs_clusters[:, 0] == combs_clusters[:, 1])[0]\n",
        "            n_pairs += len(combs)\n",
        "            n_same_cluster_pairs += len(same_cluster_pairs)\n",
        "        pw_recall = n_same_cluster_pairs / n_pairs\n",
        "        return pw_recall\n",
        "\n",
        "\n",
        "class ROCWClustering(BaseClustering):\n",
        "\n",
        "    \"\"\"Approximated rank-order clustering implemented using Chinese Whispers algorithm.\n",
        "\n",
        "    Using rank-order distances generate a graph, and feed this graph to ChineseWhispers\n",
        "     algorithm for clustering.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, k, metric, n_iteration, algorithm):\n",
        "        super().__init__()\n",
        "        self.k = k\n",
        "        self.metric = metric\n",
        "        self.n_iteration = n_iteration\n",
        "        self.knn_algorithm = algorithm\n",
        "\n",
        "    def fit_predict(self, X):\n",
        "        if len(X) > self.k:\n",
        "            graph = ROGraph(self.k, self.metric, algorithm=self.knn_algorithm)\n",
        "        else:\n",
        "            graph = ROGraph(len(X), self.metric, algorithm=self.knn_algorithm)\n",
        "        adjacency_mat = graph.generate_graph(X)\n",
        "        clusterer = ChineseWhispersClustering(self.n_iteration)\n",
        "        labels = clusterer.fit_predict(adjacency_mat)\n",
        "        return labels\n",
        "\n",
        "\n",
        "class ChineseWhispersClustering:\n",
        "\n",
        "    def __init__(self, n_iteration=5):\n",
        "        self.n_iteration = n_iteration\n",
        "        self.adjacency_mat_ = None\n",
        "        self.labels_ = None\n",
        "\n",
        "    def fit_predict(self, adjacency_mat):\n",
        "\n",
        "        \"\"\"Fits and returns labels for samples\"\"\"\n",
        "\n",
        "        n_nodes = adjacency_mat.shape[0]\n",
        "        indices = np.arange(n_nodes)\n",
        "        labels_mat = np.arange(n_nodes)\n",
        "        for _ in range(self.n_iteration):\n",
        "            np.random.shuffle(indices)\n",
        "            for ind in indices:\n",
        "                weights = adjacency_mat[ind]\n",
        "                winner_label = self._find_winner_label(weights, labels_mat)\n",
        "                labels_mat[ind] = winner_label\n",
        "        self.adjacency_mat_ = adjacency_mat\n",
        "        self.labels_ = labels_mat\n",
        "        return labels_mat\n",
        "\n",
        "    @staticmethod\n",
        "    def _find_winner_label(node_weights, labels_mat):\n",
        "        adjacent_nodes_indices = np.where(node_weights > 0)[0]\n",
        "        adjacent_nodes_labels = labels_mat[adjacent_nodes_indices]\n",
        "        unique_labels = np.unique(adjacent_nodes_labels)\n",
        "        label_weights = np.zeros(len(unique_labels))\n",
        "        for ind, label in enumerate(unique_labels):\n",
        "            indices = np.where(adjacent_nodes_labels == label)\n",
        "            weight = np.sum(node_weights[adjacent_nodes_indices[indices]])\n",
        "            label_weights[ind] = weight\n",
        "        winner_label = unique_labels[np.argmax(label_weights)]\n",
        "        return winner_label\n",
        "\n",
        "\n",
        "class ROGraph:\n",
        "\n",
        "    def __init__(self, k, metric, algorithm):\n",
        "\n",
        "        self.k = k\n",
        "        self.metric = metric\n",
        "        self.knn_algorithm = algorithm\n",
        "        self.adjacency_mat_ = None\n",
        "\n",
        "    @property\n",
        "    def adjacency_mat(self):\n",
        "        return self.adjacency_mat_\n",
        "\n",
        "    def generate_graph(self, X):\n",
        "        order_lists = self._get_knns(X)\n",
        "        pw_distances = self._generate_normalized_pw_distances(order_lists)\n",
        "        adjacency_mat = self._generate_adjacency_mat(pw_distances)\n",
        "        return adjacency_mat\n",
        "\n",
        "    def _get_knns(self, X):\n",
        "\n",
        "        \"\"\"Generates order lists and absolute distances of k-nearest-neighbors\n",
        "            for each data point.\n",
        "        \"\"\"\n",
        "\n",
        "        nbrs = NearestNeighbors(n_neighbors=self.k,\n",
        "                                algorithm=self.knn_algorithm,\n",
        "                                metric=self.metric).fit(X)\n",
        "        _, order_lists = nbrs.kneighbors(X)\n",
        "        return order_lists\n",
        "\n",
        "    def _generate_normalized_pw_distances(self, order_lists):\n",
        "        n_samples = len(order_lists)\n",
        "        combs = itertools.combinations([i for i in range(n_samples)], 2)\n",
        "        pw_distances = np.zeros((n_samples, n_samples))\n",
        "        for ind1, ind2 in combs:\n",
        "            order_list_1, order_list_2 = order_lists[ind1], order_lists[ind2]\n",
        "            pw_dist = self._calc_pw_dist(ind1, ind2, order_list_1, order_list_2)\n",
        "            pw_distances[ind1, ind2] = pw_dist\n",
        "        pw_distances = pw_distances / pw_distances.max()\n",
        "        pw_distances = pw_distances + pw_distances.T\n",
        "        return pw_distances\n",
        "\n",
        "    def _generate_adjacency_mat(self, pw_distances):\n",
        "        adjacency_mat = self._dist2adjacency(pw_distances)\n",
        "        self.adjacency_mat_ = adjacency_mat\n",
        "        return adjacency_mat\n",
        "\n",
        "    @staticmethod\n",
        "    def _dist2adjacency(distances):\n",
        "        mask_mat = np.zeros(distances.shape)\n",
        "        mask_mat[np.where(distances > 0)] = 1\n",
        "        adjacency_mat = (1 - distances) * mask_mat\n",
        "        return adjacency_mat\n",
        "\n",
        "    def _calc_pw_dist(self, ind_a, ind_b, order_list_a, order_list_b):\n",
        "        pw_dist = 0.0\n",
        "        if np.any(np.intersect1d(order_list_a, order_list_b)):\n",
        "            order_b_in_a, order_a_in_b = self._calc_orders(ind_a, ind_b, order_list_a, order_list_b)\n",
        "            d_m_ab = self._calc_dm(order_list_a, order_list_b, order_b_in_a)\n",
        "            d_m_ba = self._calc_dm(order_list_b, order_list_a, order_a_in_b)\n",
        "            min_of_two = min(order_a_in_b, order_b_in_a)\n",
        "            pw_dist = (d_m_ab + d_m_ba) / min_of_two\n",
        "        return pw_dist\n",
        "\n",
        "    def _calc_orders(self, ind_a, ind_b, order_list_a, order_list_b):\n",
        "        order_b_in_a = np.where(order_list_a == ind_b)[0]\n",
        "        if not np.any(order_b_in_a):\n",
        "            order_b_in_a = self.k\n",
        "        else:\n",
        "            order_b_in_a = order_b_in_a[0]\n",
        "        order_a_in_b = np.where(order_list_b == ind_a)[0]\n",
        "        if not np.any(order_a_in_b):\n",
        "            order_a_in_b = self.k\n",
        "        else:\n",
        "            order_a_in_b = order_a_in_b[0]\n",
        "        return order_b_in_a, order_a_in_b\n",
        "\n",
        "    def _calc_dm(self, order_list_a, order_list_b, order_b_in_a):\n",
        "        dist = 0\n",
        "        for i in range(min(self.k, order_b_in_a)):\n",
        "            sample_index = order_list_a[i]\n",
        "            if np.any(order_list_b == sample_index):\n",
        "                dist += 1 / self.k\n",
        "            else:\n",
        "                dist += 1\n",
        "        return dist"
      ],
      "metadata": {
        "id": "KnBV-9f7xdLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#face utils\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Face-related utils needed for clustering faces.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import os\n",
        "import re\n",
        "import logging\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import skimage.io as skio\n",
        "import skimage.color as skcolor\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import dlib\n",
        "from collections import OrderedDict\n",
        "import cv2\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "class FaceNet:\n",
        "\n",
        "    \"\"\"FaceNet class containing the facenet model ready to generate embeddings\n",
        "    for faces.\n",
        "\n",
        "    Naming rules:\n",
        "        self.name : externally given parameters (as argument), and public\n",
        "         parameters\n",
        "        self.name_ : internally created, non-public parameters\n",
        "        self.name() : public methods\n",
        "        self._name() : internal methods\n",
        "\n",
        "    Public methods:\n",
        "        generate_embeddings_from_images: given images, generates 512D embeddings.\n",
        "        generate_embeddings_from_paths: generates embeddings for images found at\n",
        "            specified paths.\n",
        "        plot_2d_embeddings: plots generated embeddings for generated embeddings in 2D space,\n",
        "            after performing dimension reduction using 'TSNE' method.\n",
        "        clean: closes the running tensorflow Session.\n",
        "\n",
        "    Attributes:\n",
        "        sess_: tensorflow session that contains compiled pretrained facenet model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 model_path,\n",
        "                 batch_size,\n",
        "                 image_size,\n",
        "                 do_prewhiten,\n",
        "                 do_crop):\n",
        "\n",
        "        \"\"\"Initialize a FaceNet model.\n",
        "\n",
        "      \n",
        "        \"\"\"\n",
        "\n",
        "        self.logger_ = self._initialize_logger()\n",
        "       # self.model_dir_ = os.path.join(os.path.dirname(__file__) + '/models/', model_path)\n",
        "        self.model_dir_ = '/content/drive/MyDrive/Clustering/models'\n",
        "        self.logger_.info('Model dir: {}'.format(self.model_dir_))\n",
        "        self.sess_, self.graph_ = self._load_model()\n",
        "        self.closed_ = False\n",
        "        (self.images_placeholder_,\n",
        "         self.embeddings_tensor_,\n",
        "         self.phase_train_placeholder_) = self._get_tensors()\n",
        "        self.batch_size = batch_size\n",
        "        self.image_size = image_size\n",
        "        self.do_prewhiten = do_prewhiten\n",
        "        self.do_crop = do_crop\n",
        "\n",
        "    @staticmethod\n",
        "    def _initialize_logger():\n",
        "\n",
        "        \"\"\"Initializes a console logger for logging purposes.\"\"\"\n",
        "\n",
        "        console_handler = logging.StreamHandler()\n",
        "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "        console_handler.setFormatter(formatter)\n",
        "        logger = logging.getLogger('FaceNet')\n",
        "        logger.setLevel(logging.INFO)\n",
        "        logger.addHandler(console_handler)\n",
        "        return logger\n",
        "\n",
        "    def _load_model(self):\n",
        "\n",
        "        \"\"\"Loads and compiles pretrained model's graph and associated tensors.\n",
        "\n",
        "        Returns:\n",
        "            a tensorflow session that contains the loaded and compiled model.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If pretrained model not found or more than one model exists\n",
        "                in self.model_dir_.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            meta_file, ckpt_file = self._get_model_filenames(self.model_dir_)\n",
        "        except Exception as e:\n",
        "            self.logger_.error(e)\n",
        "            raise e\n",
        "\n",
        "        sess =tf.compat.v1.Session()\n",
        "        #tf.compat.v1.disable_v2_behavior()\n",
        "\n",
        "        saver = tf.train.import_meta_graph(os.path.join(self.model_dir_, meta_file), input_map=None)\n",
        "        saver.restore(sess, os.path.join(self.model_dir_, ckpt_file))\n",
        "        self.logger_.info(\"Model loaded. Session is ready.\")\n",
        "        return sess, sess.graph\n",
        "\n",
        "    def _get_tensors(self):\n",
        "\n",
        "        \"\"\"Returns needed tensors: placeholders and embeddings tensor.\"\"\"\n",
        "\n",
        "        images_placeholder = self.graph_.get_tensor_by_name(\"input:0\")\n",
        "        embeddings = self.graph_.get_tensor_by_name(\"embeddings:0\")\n",
        "        phase_train_placeholder = self.graph_.get_tensor_by_name(\"phase_train:0\")\n",
        "\n",
        "        return images_placeholder, embeddings, phase_train_placeholder\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_model_filenames(model_dir):\n",
        "\n",
        "        \"\"\"Searches specified paths for model files.\n",
        "\n",
        "        Returns:\n",
        "            Model's meta file and checkpoint file\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If model's meta file not found or more than one meta file exists.\n",
        "        \"\"\"\n",
        "\n",
        "        files = os.listdir(model_dir)\n",
        "        meta_files = [s for s in files if s.endswith('.meta')]\n",
        "        print(meta_files)\n",
        "        if len(meta_files) == 0:\n",
        "            raise ValueError('No meta file found in the model directory (%s)' % model_dir)\n",
        "        elif len(meta_files) > 1:\n",
        "            raise ValueError('There should not be more than one meta file in\\\n",
        "             the model directory (%s)'\n",
        "                             % model_dir)\n",
        "        meta_file = meta_files[0]\n",
        "        ckpt = tf.train.get_checkpoint_state(model_dir)\n",
        "        if ckpt and ckpt.model_checkpoint_path:\n",
        "            ckpt_file = os.path.basename(ckpt.model_checkpoint_path)\n",
        "            return meta_file, ckpt_file\n",
        "\n",
        "        meta_files = [s for s in files if '.ckpt' in s]\n",
        "        max_step = -1\n",
        "        for f in files:\n",
        "            step_str = re.match(r'(^model-[\\w\\- ]+.ckpt-(\\d+))', f)\n",
        "            if step_str is not None and len(step_str.groups()) >= 2:\n",
        "                step = int(step_str.groups()[1])\n",
        "                if step > max_step:\n",
        "                    max_step = step\n",
        "                    ckpt_file = step_str.groups()[0]\n",
        "        return meta_file, ckpt_file\n",
        "\n",
        "    def generate_embeddings_from_images(self, images):\n",
        "\n",
        "        \"\"\"Generate embeddings for given images.\n",
        "\n",
        "        Args:\n",
        "            images: array of RGB images.\n",
        "\n",
        "        Returns:\n",
        "            array of 512-dimensional embeddings generated for given images.\n",
        "        \"\"\"\n",
        "\n",
        "        preprocessed_images = self._preprocess_images(images)\n",
        "        emb_array = self._generate_embeddings(preprocessed_images)\n",
        "\n",
        "        return emb_array\n",
        "\n",
        "    def generate_embeddings_from_paths(self, image_paths):\n",
        "\n",
        "        \"\"\"Generate embeddings for images found in specified paths.\n",
        "\n",
        "        Args:\n",
        "            image_paths: list of complete paths to images.\n",
        "\n",
        "        Returns:\n",
        "            generated 512-dimensional embeddings for images found on paths.\n",
        "        \"\"\"\n",
        "\n",
        "        embedding_size = self.embeddings_tensor_.get_shape()[1]\n",
        "\n",
        "        # Run forward pass to calculate embeddings\n",
        "        self.logger_.info('Calculating embeddings ...')\n",
        "        nrof_images = len(image_paths)\n",
        "        nrof_batches_per_epoch = int(np.ceil(1.0 * nrof_images / self.batch_size))\n",
        "        emb_array = np.zeros((nrof_images, embedding_size))\n",
        "\n",
        "        for i in range(nrof_batches_per_epoch):\n",
        "            start_index = i * self.batch_size\n",
        "            end_index = min((i + 1) * self.batch_size, nrof_images)\n",
        "            paths_batch = image_paths[start_index: end_index]\n",
        "            images = self._load_data_from_paths(paths_batch)\n",
        "            emb_array[start_index: end_index, :] = self._generate_batch_embeddings(images)\n",
        "\n",
        "        return emb_array\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_2d_embeddings(embs, labels):\n",
        "\n",
        "        \"\"\"Plots embeddings after dimension reduction.\n",
        "\n",
        "        For explatory tasks only.\n",
        "\n",
        "        Args:\n",
        "            embs: generated embeddings for faces.\n",
        "            labels: labels for given embeddings.\n",
        "        \"\"\"\n",
        "\n",
        "        transformed = TSNE(n_components=2).fit_transform(embs)\n",
        "        labels_set = np.unique(labels)\n",
        "\n",
        "        fig = plt.figure(figsize=(8, 8))\n",
        "        ax = fig.add_subplot(1, 1, 1)\n",
        "        ax.set_xlabel('Transformed dim 1', fontsize=15)\n",
        "        ax.set_ylabel('Transformed dim 2', fontsize=15)\n",
        "        ax.set_title('2D TSNE', fontsize=20)\n",
        "        tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14),\n",
        "                     (255, 187, 120), (44, 160, 44), (152, 223, 138),\n",
        "                     (214, 39, 40), (255, 152, 150), (148, 103, 189),\n",
        "                     (197, 176, 213), (140, 86, 75), (196, 156, 148),\n",
        "                     (227, 119, 194), (247, 182, 210), (127, 127, 127),\n",
        "                     (199, 199, 199), (188, 189, 34), (219, 219, 141),\n",
        "                     (23, 190, 207), (158, 218, 229)]\n",
        "        colors = [(i[0] / 255., i[1] / 255., i[2] / 255.)\n",
        "                  for i in tableau20][: len(labels_set)]\n",
        "\n",
        "        for label, color in zip(labels_set, colors):\n",
        "            indices_to_keep = np.where(labels == label)[0]\n",
        "            samples = transformed[indices_to_keep]\n",
        "            ax.scatter(samples[:, 0], samples[:, 1], color=color, label=label)\n",
        "\n",
        "        ax.legend(labels_set)\n",
        "        ax.grid()\n",
        "        plt.show()\n",
        "        return\n",
        "\n",
        "    def clean(self):\n",
        "\n",
        "        \n",
        "        self.sess_.close()\n",
        "        self.closed_ = True\n",
        "\n",
        "    def _generate_embeddings(self, images):\n",
        "\n",
        "        \"\"\"Generates embeddings for given images.\n",
        "\n",
        "        Args:\n",
        "            images: preprocessed images.\n",
        "\n",
        "        Returns:\n",
        "            generated embeddings.\n",
        "        \"\"\"\n",
        "\n",
        "        embedding_size = self.embeddings_tensor_.get_shape()[1]\n",
        "\n",
        "        # Run forward pass to calculate embeddings\n",
        "        nrof_images = images.shape[0]\n",
        "        nrof_batches_per_epoch = int(np.ceil(1.0 * nrof_images / self.batch_size))\n",
        "        emb_array = np.zeros((nrof_images, embedding_size))\n",
        "\n",
        "        for i in range(nrof_batches_per_epoch):\n",
        "            start_index = i * self.batch_size\n",
        "            end_index = min((i + 1) * self.batch_size, nrof_images)\n",
        "            imgs = images[start_index: end_index]\n",
        "            emb_array[start_index: end_index, :] = self._generate_batch_embeddings(imgs)\n",
        "\n",
        "        return emb_array\n",
        "\n",
        "    def _generate_batch_embeddings(self, batch):\n",
        "\n",
        "        \"\"\"One run over a batch, that generates embeddings for given batch.\n",
        "\n",
        "        Args:\n",
        "            batch: array of preprocessed images.\n",
        "\n",
        "        Returns:\n",
        "            generated embeddings\n",
        "        \"\"\"\n",
        "\n",
        "        feed_dict = {self.images_placeholder_: batch, self.phase_train_placeholder_: False}\n",
        "        return self.sess_.run(self.embeddings_tensor_, feed_dict=feed_dict)\n",
        "\n",
        "    def _load_data_from_paths(self, image_paths):\n",
        "\n",
        "        \"\"\"Loads images in RGB, and returns preprocessed images.\n",
        "\n",
        "        Args:\n",
        "            image_paths: [path1, path2, ...]\n",
        "            Note: images must be aligned faces.\n",
        "\n",
        "        Returns:\n",
        "            preprocessed images.\n",
        "        \"\"\"\n",
        "\n",
        "        nrof_samples = len(image_paths)\n",
        "        images = np.zeros((nrof_samples, self.image_size, self.image_size, 3))\n",
        "\n",
        "        for i in range(nrof_samples):\n",
        "            image_path = os.path.expanduser(image_paths[i])\n",
        "            img = skio.imread(image_path)\n",
        "            if not np.any(img):\n",
        "                print('image not found.')\n",
        "                continue\n",
        "            img = self._preprocess_image(image=img)\n",
        "            images[i, :, :, :] = img\n",
        "        return images\n",
        "\n",
        "    def _preprocess_images(self, images):\n",
        "\n",
        "        \"\"\"Preprocesses array of images.\n",
        "\n",
        "        Note: Pass aligned faces of size (182, 182) to this method\n",
        "\n",
        "\n",
        "         if self.image_size is (160, 160)\n",
        "         for performance maintaining.\n",
        "\n",
        "        Args:\n",
        "            images: array of RGB images.\n",
        "\n",
        "        Returns:\n",
        "            preprocessed images: cropped and prewhitened.\n",
        "        \"\"\"\n",
        "\n",
        "        nrof_samples = len(images)\n",
        "        imgs = np.zeros((nrof_samples, self.image_size, self.image_size, 3))\n",
        "        for i, img in enumerate(images):\n",
        "            img = self._preprocess_image(image=img)\n",
        "            imgs[i, :, :, :] = img\n",
        "        return imgs\n",
        "\n",
        "    def _preprocess_image(self, image):\n",
        "\n",
        "        \"\"\"Preprocess image for feeding to model.\n",
        "\n",
        "        Note: Pass aligned face of size (182, 182) to this method if self.image_size is (160, 160)\n",
        "         for performance maintaining.\n",
        "\n",
        "        Args:\n",
        "            image: RGB image to preprocess.\n",
        "\n",
        "        Returns:\n",
        "            prewhitened and cropped image.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.do_prewhiten:\n",
        "            image = self._prewhiten(image)\n",
        "        if self.do_crop:\n",
        "            image = self._crop(image, self.image_size)\n",
        "        return image\n",
        "\n",
        "    @staticmethod\n",
        "    def _prewhiten(x):\n",
        "\n",
        "        \"\"\"Input image standardization.\n",
        "\n",
        "        This makes predictions much accurate than using non-whitened images.\n",
        "\n",
        "        Args:\n",
        "            x: input image\n",
        "\n",
        "        Returns:\n",
        "            whitened image\n",
        "        \"\"\"\n",
        "\n",
        "        # mean = np.mean(x)\n",
        "        # std = np.std(x)\n",
        "        # std_adj = np.maximum(std, 1.0 / np.sqrt(x.size))\n",
        "        # y = np.multiply(np.subtract(x, mean), 1 / std_adj)\n",
        "        y = (x - 127.5) / 128\n",
        "        return y\n",
        "\n",
        "    @staticmethod\n",
        "    def _crop(image, image_size):\n",
        "\n",
        "        \"\"\"Crops given image and returns central (image_size, image_size) region.\"\"\"\n",
        "\n",
        "        if image.shape[1] > image_size:\n",
        "            sz1 = int(image.shape[1] // 2)\n",
        "            sz2 = int(image_size // 2)\n",
        "            (h, v) = (0, 0)\n",
        "            image = image[(sz1 - sz2 + v):(sz1 + sz2 + v),\n",
        "                          (sz1 - sz2 + h):(sz1 + sz2 + h), :]\n",
        "        return image\n",
        "\n",
        "\n",
        "class FaceDetector:\n",
        "\n",
        "    \"\"\"A class for detecting faces in images or frames.\n",
        "\n",
        "    Give image to 'detect_faces' method, and it will return all faces as an array.\n",
        "\n",
        "    Public methods:\n",
        "        detect_faces: detects faces in given image.\n",
        "\n",
        "    Attributes:\n",
        "        detector_: dlib's HOG-based frontal face detector, if cnn_path is None. else the given model will be used.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cnn_path=None):\n",
        "        if cnn_path is None:\n",
        "            self.detector_ = dlib.get_frontal_face_detector()\n",
        "        else:\n",
        "            self.detector_ = dlib.cnn_face_detection_model_v1(cnn_path)\n",
        "\n",
        "    def detect_faces(self, image):\n",
        "        \"\"\"Detects faces in given image.\n",
        "\n",
        "        Args:\n",
        "            image: RGB image\n",
        "\n",
        "        Returns:\n",
        "            detected faces: an array of rectangles.\n",
        "        \"\"\"\n",
        "\n",
        "        # resized_image = self._resize_image(image, out_pixels_wide=800)\n",
        "        # gray = (skcolor.rgb2gray(image) * 255).astype(np.uint8)\n",
        "\n",
        "        rects = self.detector_(image, 0)\n",
        "        return rects\n",
        "\n",
        "    @staticmethod\n",
        "    def _resize_image(img, out_pixels_wide):\n",
        "        ratio = out_pixels_wide / img.shape[1]\n",
        "        dim = (int(out_pixels_wide), int(img.shape[0] * ratio))\n",
        "        resized = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)\n",
        "        return resized\n",
        "\n",
        "\n",
        "class FaceAligner:\n",
        "\n",
        "    \"\"\"Aligns faces, use aligned faces for generating embeddings via FaceNet.\n",
        "\n",
        "    Public methods:\n",
        "        align: aligns faces found in given image.\n",
        "\n",
        "    Attributes:\n",
        "        predictor_: dlib's facial shape predictor (5 landmarks).\n",
        "        landmarks_idxs_: Positions for output landmarks.\n",
        "        desired_left_eye: Desired output left eye position, given in a (x, y) tuple.\n",
        "            Percentages between (0.2, 0.4), With 20% you’ll basically be getting a\n",
        "            “zoomed in” view of the face, whereas with larger values the face will\n",
        "            appear more “zoomed out.”\n",
        "        desired_face_width: Output images width, in pixels.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 predictor_path,\n",
        "                 desired_left_eye=(0.35, 0.35),\n",
        "                 desired_face_width=182):\n",
        "        \"\"\"Init a FaceAligner instance.\n",
        "\n",
        "        Args:\n",
        "            predictor_path: path to dlib's facial shape predictor (5 landmarks)\n",
        "            desired_left_eye: Desired output left eye position, given in a (x, y) tuple.\n",
        "                Percentages between (0.2, 0.4), With 20% you’ll basically be getting a\n",
        "                “zoomed in” view of the face, whereas with larger values the face will\n",
        "                appear more “zoomed out.”\n",
        "            desired_face_width: Output images width, in pixels.\n",
        "        \"\"\"\n",
        "\n",
        "        # predictor_dir = os.path.join(os.path.dirname(__file__) + '/models/shape_predictor/',\n",
        "        #                              predictor_path)\n",
        "        self.predictor_ = dlib.shape_predictor(predictor_path)\n",
        "        FACIAL_LANDMARKS_5_IDXS = OrderedDict([\n",
        "            (\"right_eye\", (2, 3)),\n",
        "            (\"left_eye\", (0, 1)),\n",
        "            (\"nose\", (4,))\n",
        "        ])\n",
        "        self.landmarks_idxs_ = FACIAL_LANDMARKS_5_IDXS\n",
        "        self.desired_left_eye = desired_left_eye\n",
        "        self.desired_right_eye = (1.0 - self.desired_left_eye[0], self.desired_left_eye[1])\n",
        "        self.desired_face_width = desired_face_width\n",
        "        self.desired_face_height = desired_face_width\n",
        "\n",
        "    def align(self, image, rects):\n",
        "        \"\"\"Aligns the faces specified by rects in image.\n",
        "\n",
        "        Args:\n",
        "            image: The RGB input image.\n",
        "            rects: The bounding box rectangles produced by dlib’s HOG face detector.\n",
        "\n",
        "        Returns:\n",
        "            a list of aligned faces.\n",
        "        \"\"\"\n",
        "\n",
        "        gray = skcolor.rgb2gray(image).astype(np.uint8)\n",
        "        outputs = list()\n",
        "        for rect in rects:\n",
        "            shape = self.predictor_(gray, rect.rect)\n",
        "            predicted_coords = self._shape_to_np(shape)\n",
        "\n",
        "            tform_mat = self._calc_transform_mat(predicted_coords)\n",
        "\n",
        "            output = self._apply_transformation(image, tform_mat)\n",
        "            outputs.append(output)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def _apply_transformation(self, image, M):\n",
        "        \"\"\"Applies affine transform to given image using given matrix.\n",
        "\n",
        "        Args:\n",
        "            image: RGB image\n",
        "            M: transformation matrix\n",
        "\n",
        "        Returns:\n",
        "            Aligned face\n",
        "        \"\"\"\n",
        "\n",
        "        (w, h) = (self.desired_face_width, self.desired_face_height)\n",
        "        output = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC)\n",
        "        return output\n",
        "\n",
        "    def _calc_transform_mat(self, predicted_coords):\n",
        "        \"\"\"Generates transorfmation matrix.\n",
        "\n",
        "        Args:\n",
        "            predicted_coords: predicted landmarks' coordinates\n",
        "\n",
        "        Returns:\n",
        "            Transorm matrix, should given to warpAffine\n",
        "        \"\"\"\n",
        "\n",
        "        eyes_center, angle, scale = self._calc_mat_params(predicted_coords)\n",
        "        M = cv2.getRotationMatrix2D(eyes_center, angle, scale)\n",
        "\n",
        "        tX = self.desired_face_width * 0.5\n",
        "        tY = self.desired_face_height * self.desired_left_eye[1]\n",
        "        M[0, 2] += (tX - eyes_center[0])\n",
        "        M[1, 2] += (tY - eyes_center[1])\n",
        "        return M\n",
        "\n",
        "    def _calc_mat_params(self, predicted_coords):\n",
        "        (left_eye_start, left_eye_end) = self.landmarks_idxs_[\"left_eye\"]\n",
        "        (right_eye_start, right_eye_end) = self.landmarks_idxs_[\"right_eye\"]\n",
        "\n",
        "        left_eye_pts = predicted_coords[left_eye_start: left_eye_end + 1]\n",
        "        right_eye_pts = predicted_coords[right_eye_start: right_eye_end + 1]\n",
        "\n",
        "        left_eye_center = left_eye_pts.mean(axis=0).astype(\"int\")\n",
        "        right_eye_center = right_eye_pts.mean(axis=0).astype(\"int\")\n",
        "\n",
        "        dY = right_eye_center[1] - left_eye_center[1]\n",
        "        dX = right_eye_center[0] - left_eye_center[0]\n",
        "        angle = np.degrees(np.arctan2(dY, dX)) - 180\n",
        "\n",
        "        desired_right_eye_x = 1.0 - self.desired_left_eye[0]\n",
        "\n",
        "        dist = np.sqrt((dX ** 2) + (dY ** 2))\n",
        "        desired_dist = (desired_right_eye_x - self.desired_left_eye[0])\n",
        "        desired_dist *= self.desired_face_width\n",
        "        scale = desired_dist / dist\n",
        "\n",
        "        eyes_center = ((left_eye_center[0] + right_eye_center[0]) // 2,\n",
        "                       (left_eye_center[1] + right_eye_center[1]) // 2)\n",
        "        return eyes_center, angle, scale\n",
        "\n",
        "    @staticmethod\n",
        "    def _shape_to_np(shape, dtype=\"int\"):\n",
        "\n",
        "        # Initialize the list of (x, y)-coordinates\n",
        "        coords = np.zeros((shape.num_parts, 2), dtype=dtype)\n",
        "\n",
        "        # loop over all facial landmarks and convert them\n",
        "        # to a 2-tuple of (x, y)-coordinates\n",
        "        for i in range(0, shape.num_parts):\n",
        "            coords[i] = (shape.part(i).x, shape.part(i).y)\n",
        "\n",
        "        # return the list of (x, y)-coordinates\n",
        "        return coords"
      ],
      "metadata": {
        "id": "09635avhxnon",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17a9f465-87a5-4de3-b446-aaaa5deeae73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfA6-3Hv891Q",
        "outputId": "96c9b6c9-d214-426f-9121-20efc2ba1926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://dlib.net/files/mmod_human_face_detector.dat.bz2 http://dlib.net/files/shape_predictor_5_face_landmarks.dat.bz2 https://drive.google.com/file/d/1EXPBSXwTaqrSC0OhUdXNmKSh9qJUQ55-/view\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLRAksgo9EAY",
        "outputId": "fb1795b8-5224-476c-dfff-c420f4fb91d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-28 10:18:19--  http://dlib.net/files/mmod_human_face_detector.dat.bz2\n",
            "Resolving dlib.net (dlib.net)... 107.180.26.78\n",
            "Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 694709 (678K)\n",
            "Saving to: ‘mmod_human_face_detector.dat.bz2’\n",
            "\n",
            "\r          mmod_huma   0%[                    ]       0  --.-KB/s               \rmmod_human_face_det 100%[===================>] 678.43K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2022-08-28 10:18:20 (6.99 MB/s) - ‘mmod_human_face_detector.dat.bz2’ saved [694709/694709]\n",
            "\n",
            "--2022-08-28 10:18:20--  http://dlib.net/files/shape_predictor_5_face_landmarks.dat.bz2\n",
            "Reusing existing connection to dlib.net:80.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5706710 (5.4M)\n",
            "Saving to: ‘shape_predictor_5_face_landmarks.dat.bz2’\n",
            "\n",
            "shape_predictor_5_f 100%[===================>]   5.44M  24.6MB/s    in 0.2s    \n",
            "\n",
            "2022-08-28 10:18:20 (24.6 MB/s) - ‘shape_predictor_5_face_landmarks.dat.bz2’ saved [5706710/5706710]\n",
            "\n",
            "--2022-08-28 10:18:20--  https://drive.google.com/file/d/1EXPBSXwTaqrSC0OhUdXNmKSh9qJUQ55-/view\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.215.139, 173.194.215.113, 173.194.215.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.215.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘view’\n",
            "\n",
            "view                    [ <=>                ]  66.18K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2022-08-28 10:18:21 (7.47 MB/s) - ‘view’ saved [67770]\n",
            "\n",
            "FINISHED --2022-08-28 10:18:21--\n",
            "Total wall clock time: 2.1s\n",
            "Downloaded: 3 files, 6.2M in 0.3s (19.0 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#face clustering\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Face clustering module.\n",
        "\n",
        "   This file contains FaceClustering class.\n",
        "   It uses facenet's pretrained CNN for feature extraction, and uses predefined\n",
        "    clustering algorithms for performing clustering on images or video files.\n",
        "\n",
        "   Instantiate an object from this class, tune the model on your own data if\n",
        "    needed, and enjoy clustered faces.\n",
        "\n",
        "\n",
        "   You can use the model in context manager mode, but the session will\n",
        "    terminate at the end of the 'with' block. When using in standard way,\n",
        "    the background tensorflow session wouldn't terminate in order to avoid\n",
        "    redundant operations.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "class FaceClustering:\n",
        "\n",
        "    \"\"\"Performs clustering on faces, based on embeddings, images or video.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, clustering_alg='dbscan'):\n",
        "\n",
        "        assert clustering_alg in ('cw', 'dbscan')\n",
        "\n",
        "        if clustering_alg == 'cw':\n",
        "            self.clusterer_ = ROCWClustering(k=config.ROCW_K,\n",
        "                                             metric=config.ROCW_METRIC,\n",
        "                                             n_iteration=config.ROCW_N_ITERATION,\n",
        "                                             algorithm=config.ROCW_ALGORITHM)\n",
        "        else:\n",
        "            self.clusterer_ = DBSCAN(eps=config.DBSCAN_EPS,\n",
        "                                     min_samples=config.DBSCAN_MIN_SAMPLES,\n",
        "                                     metric=config.DBSCAN_METRIC)\n",
        "        print('clustering algorithm created.')\n",
        "\n",
        "        self.facenet_model_ = FaceNet(model_path=config.FACENET_MODEL_PATH,\n",
        "                                      batch_size=config.FACENET_BATCH_SIZE,\n",
        "                                      image_size=config.FACENET_IMAGE_SIZE,\n",
        "                                      do_prewhiten=config.FACENET_DO_PREWHITEN,\n",
        "                                      do_crop=config.FACENET_DO_CROP)\n",
        "        print('facenet is ready.')\n",
        "\n",
        "        self.face_detector_ = FaceDetector(cnn_path=config.DLIB_FACE_DETECTOR_PATH)\n",
        "        print('face detector is ready.')\n",
        "\n",
        "        self.face_aligner_ = FaceAligner(predictor_path=config.DLIB_FACIAL_SHAPE_PREDICTOR_PATH,\n",
        "                                         desired_left_eye=config.FACE_ALIGNER_LEFT_EYE,\n",
        "                                         desired_face_width=config.FACE_ALIGNER_OUTPUT_SIZE)\n",
        "        print('face aligner is ready.')\n",
        "\n",
        "        self.valid_images = config.VALID_IMAGES\n",
        "        self.max_input_height = config.MAX_INPUT_HEIGHT\n",
        "        self.corr_th_between_frames = config.CORRELATION_THRESHOLD_BETWEEN_FRAMES\n",
        "        self.vid_sampling_period = config.VIDEO_SAMPLING_PERIOD\n",
        "\n",
        "    def do_clustering(self, X):\n",
        "\n",
        "        \"\"\"Clustering on given samples.\n",
        "\n",
        "        Args:\n",
        "            X: an array of arrays, each row is a sample and columns are\n",
        "             features.\n",
        "\n",
        "        Returns:\n",
        "            an array that contains predicted labels for given X.\n",
        "        \"\"\"\n",
        "\n",
        "        labels = self.clusterer_.fit_predict(X)\n",
        "        return labels\n",
        "\n",
        "    def do_clustering_on_images(self, images_dir):\n",
        "\n",
        "        \"\"\"Clustering on images found on specified folder.\n",
        "\n",
        "        Finds and aligns faces, saves faces in a new folder called \"found_faces\" on given path,\n",
        "         then performs clustering on found faces.\n",
        "\n",
        "        Args:\n",
        "            images_dir: folder of images.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary that maps each found_face_name to its label.\n",
        "        \"\"\"\n",
        "\n",
        "        faces_dir = os.path.join(images_dir, 'found_faces')\n",
        "        if not os.path.exists(faces_dir):\n",
        "            embs, faces = self._detect_align_generate_embs(images_dir)\n",
        "            if np.any(embs):\n",
        "                labels = self.clusterer_.fit_predict(embs)\n",
        "                uniques = np.unique(labels)\n",
        "                os.mkdir(faces_dir)\n",
        "                for label in uniques:\n",
        "                    label_dir = os.path.join(faces_dir, str(label))\n",
        "                    os.mkdir(label_dir)\n",
        "                    for ind in np.where(labels == label)[0]:\n",
        "                        img_path = os.path.join(label_dir, '{}.jpg'.format(ind))\n",
        "                        self._write_bgr_image(faces[ind], img_path)\n",
        "                        # skio.imsave(img_path, img_as_ubyte(faces[ind]))\n",
        "                self.draw_tsne(embs, labels, faces_dir)\n",
        "                print('results saved on ', faces_dir)\n",
        "        else:\n",
        "            print('the found_faces sub-directory already exists at ', images_dir)\n",
        "\n",
        "    def do_clustering_on_video(self, video_path, num_frames=None):\n",
        "\n",
        "        \"\"\"Clustering on given video path.\n",
        "\n",
        "        Args:\n",
        "            video_path: complete path to video.\n",
        "            num_frames: how many frames to read and process from video?\n",
        "\n",
        "        Returns:\n",
        "            a dictionary containing cropped faces for each cluster:\n",
        "                {cluster1: [face1, face2, ...], cluster2: [face1, face2], ...}\n",
        "        \"\"\"\n",
        "\n",
        "        base_dir = os.path.dirname(video_path)\n",
        "        faces_dir = os.path.join(base_dir, 'found_faces')\n",
        "        if not os.path.exists(faces_dir):\n",
        "            embs, faces = self._dag_video(video_path, num_frames)\n",
        "            if embs:\n",
        "                labels = self.clusterer_.fit_predict(embs)\n",
        "                uniques = np.unique(labels)\n",
        "                os.mkdir(faces_dir)\n",
        "                for label in uniques:\n",
        "                    label_dir = os.path.join(faces_dir, str(label))\n",
        "                    os.mkdir(label_dir)\n",
        "                    for ind in np.where(labels == label)[0]:\n",
        "                        img_path = os.path.join(label_dir, '{}.jpg'.format(ind))\n",
        "                        self._write_bgr_image(faces[ind], img_path)\n",
        "                        # skio.imsave(img_path, img_as_ubyte(faces[ind]))\n",
        "                self.draw_tsne(embs, labels, faces_dir)\n",
        "                print('results saved on ', faces_dir)\n",
        "        else:\n",
        "            print('found faces already exists.')\n",
        "\n",
        "    def show_cluster(self, cluster_dir):\n",
        "\n",
        "        \"\"\"Shows all members of given cluster in a single image.\"\"\"\n",
        "\n",
        "        face_paths = [os.path.join(cluster_dir, item) for item in os.listdir(cluster_dir) if item.endswith('.jpg')]\n",
        "        fig = self._plot_faces(face_paths)\n",
        "        fig.show()\n",
        "\n",
        "    def _dag_video(self, video_path, num_frames):\n",
        "\n",
        "        \"\"\"Detect and align the faces found in given video and generate embeddings for them.\"\"\"\n",
        "\n",
        "        vidcap = cv2.VideoCapture(video_path)\n",
        "        w, h, fps, fc = self._get_vid_specs(vidcap)\n",
        "        if num_frames is None:\n",
        "            num_frames = fc\n",
        "\n",
        "        to_skip = int(self.vid_sampling_period * fps)\n",
        "\n",
        "        embs = list()\n",
        "        faces = list()\n",
        "        last_frame = np.zeros((h, w, 3), dtype=np.int8)\n",
        "        with tqdm(total=int(num_frames / to_skip)) as pbar:\n",
        "            for frame_ind in range(num_frames):\n",
        "                frame = self._read_rgb_frame(vidcap)\n",
        "                if frame is not None:\n",
        "                    if frame_ind % to_skip == 0:\n",
        "                        if np.any(frame):\n",
        "                            if np.any(last_frame):\n",
        "                                corr = np.abs(np.corrcoef(last_frame.flatten(), frame.flatten())[0, 1])\n",
        "                            else:\n",
        "                                corr = 0\n",
        "                            if corr < self.corr_th_between_frames:\n",
        "                                aligned_faces = self._detect_align(frame)\n",
        "                                if np.any(aligned_faces):\n",
        "                                    embs.extend(self.facenet_model_.generate_embeddings_from_images(aligned_faces))\n",
        "                                    faces.extend(aligned_faces)\n",
        "                                    # for j, face in enumerate(aligned_faces):\n",
        "                                        # face_path = os.path.join(faces_dir, '{}_{}.jpg'.format(frame_ind, j))\n",
        "                                        # faces.append(face_path)\n",
        "                        last_frame = frame.copy()\n",
        "                        pbar.update(1)\n",
        "                else:\n",
        "                    raise Exception('could not read frame {}'.format(frame_ind))\n",
        "\n",
        "        return embs, faces\n",
        "\n",
        "    def _detect_align(self, image):\n",
        "\n",
        "        \"\"\"Detects faces, aligns and returns them.\"\"\"\n",
        "\n",
        "        image_height = image.shape[0]\n",
        "        # if image_height > self.max_input_height:\n",
        "        scale_factor = self.max_input_height / image_height\n",
        "        image = cv2.resize(image, dsize=None, fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "        rects = self.face_detector_.detect_faces(image)\n",
        "        if not rects:\n",
        "            return []\n",
        "        aligned_faces = self.face_aligner_.align(image, rects)\n",
        "        return aligned_faces\n",
        "\n",
        "    def _detect_align_generate_embs(self, images_dir):\n",
        "\n",
        "        \"\"\"Detects faces, aligns them and returns the generated embeddings for them.\"\"\"\n",
        "\n",
        "        image_paths = [os.path.join(images_dir, img_name)\n",
        "                       for img_name in os.listdir(images_dir) if img_name.lower().endswith(self.valid_images)]\n",
        "\n",
        "        if not image_paths:\n",
        "            print('could not find any image in ', images_dir)\n",
        "            return None, None\n",
        "\n",
        "        # face_paths = list()\n",
        "        embs = list()\n",
        "        faces = list()\n",
        "        print('processing ...')\n",
        "        with tqdm(total=len(image_paths)) as pbar:\n",
        "            for path in image_paths:\n",
        "                image = self._read_rgb_image(path)\n",
        "                aligned_faces = self._detect_align(image)\n",
        "                if aligned_faces:\n",
        "                    embs.extend(self.facenet_model_.generate_embeddings_from_images(aligned_faces))\n",
        "                    faces.extend(aligned_faces)\n",
        "                    # for j, face in enumerate(aligned_faces):\n",
        "                    #     face_path = os.path.join(face_folder, '{}_{}.jpg'.format(i, j))\n",
        "                    #     skio.imsave(face_path, face)\n",
        "                    #     face_paths.append(face_path)\n",
        "                pbar.update(1)\n",
        "        # return face_paths, np.array(embs)\n",
        "        return embs, faces\n",
        "\n",
        "    def evaluate(self, X, y_true):\n",
        "\n",
        "        \"\"\"Evaluates clustering algorithm on given data.\n",
        "\n",
        "        Args:\n",
        "            X: an array of arrays\n",
        "            y_true: 1D array, true labels for each row of X\n",
        "\n",
        "        Returns:\n",
        "            pairwise f-measure\n",
        "        \"\"\"\n",
        "\n",
        "        return self.clusterer_.score(X, y_true)\n",
        "\n",
        "    def evaluate_on_images(self, images, labels):\n",
        "\n",
        "        \"\"\"Evaluates clustering on given images.\n",
        "\n",
        "        Args:\n",
        "            images: array of rgb images.\n",
        "            labels: array of labels for given images.\n",
        "\n",
        "        Returns:\n",
        "            pairwise f-measure\n",
        "        \"\"\"\n",
        "\n",
        "        embeddings = list()\n",
        "        flipped_faces = list()\n",
        "        for image in images:\n",
        "            rects = self.face_detector_.detect_faces(image)\n",
        "            if not rects:\n",
        "                continue\n",
        "            aligned_faces = self.face_aligner_.align(image, rects)\n",
        "            for face in aligned_faces:\n",
        "                flipped_faces.append(cv2.flip(face, 1))\n",
        "            embs = self.facenet_model_.generate_embeddings_from_images(aligned_faces)\n",
        "            embeddings.extend(embs)\n",
        "        flipped_embs = self.facenet_model_.generate_embeddings_from_images(flipped_faces)\n",
        "        embeddings.extend(flipped_embs)\n",
        "\n",
        "        return self.clusterer_.score(embeddings, labels)\n",
        "\n",
        "    def _plot_faces(self, face_paths):\n",
        "        dim = int(np.sqrt(len(face_paths))) + 1\n",
        "        fig, axes = plt.subplots(nrows=dim, ncols=dim, figsize=(18, 18))\n",
        "        for i, ax in enumerate(axes.flatten()):\n",
        "            if i >= len(face_paths):\n",
        "                break\n",
        "            ax.imshow(self._read_rgb_image(face_paths[i]))\n",
        "        return fig\n",
        "\n",
        "    def __enter__(self):\n",
        "\n",
        "        \"\"\"Returns model when object enters a \"with\" block\"\"\"\n",
        "\n",
        "        return self\n",
        "\n",
        "    def __exit__(self):\n",
        "        self.facenet_model_.close()\n",
        "\n",
        "    @staticmethod\n",
        "    def draw_tsne(embs, labels, face_folder):\n",
        "        transformed = TSNE(n_components=2).fit_transform(embs)\n",
        "        labels_set = np.unique(labels)\n",
        "\n",
        "        fig = plt.figure(figsize=(12, 12))\n",
        "        ax = fig.add_subplot(1, 1, 1)\n",
        "        ax.set_xlabel('Transformed dim 1', fontsize=15)\n",
        "        ax.set_ylabel('Transformed dim 2', fontsize=15)\n",
        "        ax.set_title('2D TSNE', fontsize=20)\n",
        "        tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14),\n",
        "                     (255, 187, 120), (44, 160, 44), (152, 223, 138),\n",
        "                     (214, 39, 40), (255, 152, 150), (148, 103, 189),\n",
        "                     (197, 176, 213), (140, 86, 75), (196, 156, 148),\n",
        "                     (227, 119, 194), (247, 182, 210), (127, 127, 127),\n",
        "                     (199, 199, 199), (188, 189, 34), (219, 219, 141),\n",
        "                     (23, 190, 207), (158, 218, 229)]\n",
        "        colors = [(i[0] / 255., i[1] / 255., i[2] / 255.)\n",
        "                  for i in tableau20][: len(labels_set)]\n",
        "\n",
        "        for label, color in zip(labels_set, colors):\n",
        "            indices_to_keep = np.where(labels == label)[0]\n",
        "            samples = transformed[indices_to_keep]\n",
        "            ax.scatter(samples[:, 0], samples[:, 1], color=color, label=label)\n",
        "\n",
        "        ax.legend(labels_set)\n",
        "        ax.grid()\n",
        "        fig.savefig(os.path.join(face_folder, 'tsne.png'))\n",
        "\n",
        "    @staticmethod\n",
        "    def _read_rgb_frame(vidcap):\n",
        "        ret, frame = vidcap.read()\n",
        "        if ret:\n",
        "            return cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_vid_specs(vidcap):\n",
        "        width = int(vidcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "        fc = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        return width, height, fps, fc\n",
        "\n",
        "    @staticmethod\n",
        "    def _read_rgb_image(imag_path):\n",
        "        image = cv2.imread(imag_path)\n",
        "        return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    @staticmethod\n",
        "    def _write_bgr_image(image, path):\n",
        "        cv2.imwrite(path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))"
      ],
      "metadata": {
        "id": "eVDrG1qR2aer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#config\n",
        "from os.path import normpath, abspath, dirname, realpath, join\n",
        "\n",
        "\n",
        "class FaceKooConfig:\n",
        "\n",
        "    ROCW_K = 50\n",
        "    ROCW_METRIC = 'minkowski'\n",
        "    ROCW_N_ITERATION = 2\n",
        "    ROCW_ALGORITHM = 'auto'\n",
        "\n",
        "    # Higher 'min_samples' or lower 'eps' indicate higher density necessary to form a cluster.\n",
        "    DBSCAN_EPS = 0.4\n",
        "    DBSCAN_MIN_SAMPLES = 2\n",
        "    DBSCAN_METRIC = 'cosine'\n",
        "\n",
        "\n",
        "    #PROJECT_ROOT = normpath(abspath(dirname(dirname(realpath('')))))\n",
        "    #PROJECT_ROOT=''\n",
        "    #MODELS_DIR = join(PROJECT_ROOT, 'models')\n",
        "    MODELS_DIR='/content/drive/MyDrive/Clustering/models'\n",
        "\n",
        "    # Download from here: https://drive.google.com/file/d/1EXPBSXwTaqrSC0OhUdXNmKSh9qJUQ55-/view\n",
        "    FACENET_MODEL_PATH = join(MODELS_DIR, '20180402-114759')\n",
        "    FACENET_BATCH_SIZE = 32\n",
        "    FACENET_IMAGE_SIZE = 160\n",
        "    FACENET_DO_PREWHITEN = True\n",
        "    FACENET_DO_CROP = True\n",
        "\n",
        "    # Download dlib's face detector model and unzip it: http://dlib.net/files/mmod_human_face_detector.dat.bz2\n",
        "    DLIB_FACE_DETECTOR_PATH = join(MODELS_DIR, 'mmod_human_face_detector.dat')\n",
        "\n",
        "    # Download dlib's facial shape predictor and unzip it:\n",
        "    # http://dlib.net/files/shape_predictor_5_face_landmarks.dat.bz2\n",
        "    DLIB_FACIAL_SHAPE_PREDICTOR_PATH = join(MODELS_DIR, 'shape_predictor_5_face_landmarks.dat')\n",
        "    FACE_ALIGNER_OUTPUT_SIZE = 182\n",
        "    FACE_ALIGNER_LEFT_EYE = (0.35, 0.35)\n",
        "\n",
        "    VALID_IMAGES = ('.jpg', '.png')\n",
        "\n",
        "    # The face-detector may crash when the input image is so large, when the detector is cnn-based,\n",
        "    #   i.e. DLIB_FACE_DETECTOR_PATH is not None\n",
        "    MAX_INPUT_HEIGHT = 1024\n",
        "\n",
        "    # When processing a video, skip detection on frames that have correlation coefficient of higher than this value vs\n",
        "    #   the most recent frame\n",
        "    CORRELATION_THRESHOLD_BETWEEN_FRAMES = 0.985\n",
        "\n",
        "    # Sample video every () seconds when doing clustering on video\n",
        "    VIDEO_SAMPLING_PERIOD = 1.0"
      ],
      "metadata": {
        "id": "z_7t5zDlypVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://dlib.net/files/mmod_human_face_detector.dat.bz2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3ewYyMr60I8",
        "outputId": "41f4e5b1-9165-4931-be6a-cd938bbcfd89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-28 10:07:33--  http://dlib.net/files/mmod_human_face_detector.dat.bz2\n",
            "Resolving dlib.net (dlib.net)... 107.180.26.78\n",
            "Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 694709 (678K)\n",
            "Saving to: ‘mmod_human_face_detector.dat.bz2’\n",
            "\n",
            "mmod_human_face_det 100%[===================>] 678.43K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2022-08-28 10:07:33 (7.33 MB/s) - ‘mmod_human_face_detector.dat.bz2’ saved [694709/694709]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bz2\n",
        "def unzip_bz2_file(zipped_file_name):\n",
        "    zipfile = bz2.BZ2File(zipped_file_name)\n",
        "    data = zipfile.read(size=-1)\n",
        "    newfilepath = zipped_file_name[:-4] #discard .bz2 extension\n",
        "    open(newfilepath, 'wb').write(data)"
      ],
      "metadata": {
        "id": "c0UUGWj567af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bunzip2 /content/drive/MyDrive/Models/shape_predictor_5_face_landmarks.dat.bz2\n"
      ],
      "metadata": {
        "id": "CzQr2LuDAOHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g4EWoET7I-j7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import dlib\n",
        "import skimage.transform as sktrans\n",
        "\n",
        "#from src import FaceKooConfig, FaceClusering\n",
        "config = FaceKooConfig()\n",
        "config.MAX_INPUT_HEIGHT = 800\n",
        "facekoo = FaceClustering(config)\n",
        "\n"
      ],
      "metadata": {
        "id": "3gXVD2a1x72V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "i-7Nguvz6_aT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_dir = '/content/drive/MyDrive/Models/Images'\n",
        "facekoo.do_clustering_on_images(images_dir)"
      ],
      "metadata": {
        "id": "HRpCBYik4Qbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/eonr/ShowSegmentation/tree/master/W1%262-Keyshots/SAA_clip_key_frames"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQyZxwbLGL9d",
        "outputId": "0c32a1f2-3d8c-407d-dfca-008bd2f6fd26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-28 10:56:50--  https://github.com/eonr/ShowSegmentation/tree/master/W1%262-Keyshots/SAA_clip_key_frames\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘SAA_clip_key_frames’\n",
            "\n",
            "SAA_clip_key_frames     [ <=>                ] 224.39K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2022-08-28 10:56:50 (4.05 MB/s) - ‘SAA_clip_key_frames’ saved [229776]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/Models/20180402-114759.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFzSJ6uLGNL8",
        "outputId": "4a42c26d-0389-4243-dd95-970121489a59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Models/20180402-114759.zip\n",
            "   creating: 20180402-114759/\n",
            "  inflating: 20180402-114759/model-20180402-114759.meta  \n",
            "  inflating: 20180402-114759/20180402-114759.pb  \n",
            "  inflating: 20180402-114759/model-20180402-114759.ckpt-275.data-00000-of-00001  \n",
            "  inflating: 20180402-114759/model-20180402-114759.ckpt-275.index  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/Models/mmod_human_face_detector.dat.bz2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7SfVWJwIXnS",
        "outputId": "0b3ebb29-5949-444b-e5f4-a7d7db3ef8ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Models/mmod_human_face_detector.dat.bz2\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /content/drive/MyDrive/Models/mmod_human_face_detector.dat.bz2 or\n",
            "        /content/drive/MyDrive/Models/mmod_human_face_detector.dat.bz2.zip, and cannot find /content/drive/MyDrive/Models/mmod_human_face_detector.dat.bz2.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xf /content/drive/MyDrive/Models/mmod_human_face_detector.dat.bz2 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uh7k6-OTJ6ho",
        "outputId": "4d6cad2f-0b84-4918-d8dd-be87d97179aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar: This does not look like a tar archive\n",
            "tar: Skipping to next header\n",
            "tar: Exiting with failure status due to previous errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bzip2 -d /content/drive/MyDrive/Models/mmod_human_face_detector.dat.bz2 "
      ],
      "metadata": {
        "id": "qFBdq1cuK4A3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xYj1v_1TNhMS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}